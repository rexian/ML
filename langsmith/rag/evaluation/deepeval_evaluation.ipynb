{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFYYnYY46Nft/lOy1h5HwC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rexian/ML/blob/main/langsmith/rag/evaluation/deepeval_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5loHwWW6wdg_"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q h5py typing-extensions wheel\n",
        "!pip install -U -q chromadb tiktoken pypdf\n",
        "!pip install -U -q langchain-community langchain_huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U -q deepeval aiobotocore"
      ],
      "metadata": {
        "id": "WVeMpgAYQgjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "eQa6bi0MxNfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q faiss-cpu \tlangchain-nvidia-ai-endpoints"
      ],
      "metadata": {
        "id": "_RWQR1T0yksl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\n",
        "    \"./spaul/Deep-Learning-with-PyTorch.pdf\",\n",
        ")\n",
        "document = loader.load()"
      ],
      "metadata": {
        "id": "6m1934H7zI3c"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 20)\n",
        "text = text_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "mZ4fReVez9VV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "persist_directory = 'db'\n",
        "vectordb = Chroma.from_documents(documents=text,\n",
        "                                 collection_name=\"deepeval\",\n",
        "                                 embedding=hf,\n",
        "                                 persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "AM-CeNGp0Dhd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb.persist()\n",
        "vectordb = None"
      ],
      "metadata": {
        "id": "vLszXI9-2HH7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=hf)"
      ],
      "metadata": {
        "id": "yS8G0V6G2L2w"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()\n",
        "docs = retriever.get_relevant_documents(\"What is natural language processing?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgg0nR9K2Uo0",
        "outputId": "a5fd00fc-002d-4798-99c9-2d1c898a5fcd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-533c3e6fbc68>:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(\"What is natural language processing?\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA AI Endpoints: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA6vH7Y-2kXa",
        "outputId": "760caf60-8eb3-4b48-b774-2188096b4b17"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for NVIDIA AI Endpoints: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "template = \"\"\"\n",
        "\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say 'Ah snap homie, I ain't gonna front. I don't know.`, don't try to make up an answer.\n",
        "Use three sentences maximum, relevant analogies, and keep the answer as concise as possible.\n",
        "Use the active voice, and speak directly to the reader using concise language.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "nvidia_model = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    nvidia_model,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n",
        "\n",
        "query = \"What is deep learning?\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "iBP8QTWJ3Fw2",
        "outputId": "5f29b63e-9010-4847-dad8-bc9398c87afc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Deep learning is a type of machine learning that deals with training mathematical entities named deep neural networks on the basis of examples, allowing them to approximate highly nonlinear phenomena and capture the inherent structure of data. It's like a super-smart, data-driven approach to solving problems, where you don't always need to understand the underlying phenomenon, but can still get accurate results. Think of it like a submarine that can swim, without needing to understand the intricacies of swimming!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "persistent_client = chromadb.PersistentClient(path=\"db\")\n",
        "collection = persistent_client.get_or_create_collection(\"deepeval\")\n",
        "collection.name\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yawU7Wjw6Eu3",
        "outputId": "59a72da9-cadd-4b62-c024-b42de4d65ff8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'deepeval'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "def search(query):\n",
        "  query_embedding = hf.embed_query(query)\n",
        "\n",
        "  res = collection.query(\n",
        "    query_embeddings=[query_embedding],\n",
        "    #query_texts=[query],\n",
        "    n_results=3,  # Retrieve top-K matches\n",
        "    include=[\"documents\"]\n",
        "  )\n",
        "  result = []\n",
        "  for doc in res['documents']:\n",
        "    result.append(doc[0])\n",
        "  return  result if result else None\n",
        "\n",
        "query = \"What is tensor in deep learning?\"\n",
        "retrieval_context = search(query)\n",
        "print(retrieval_context)\n",
        "\n",
        "llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Answer the user question based on the supporting context.\n",
        "\n",
        "User Question:\n",
        "{input}\n",
        "\n",
        "Supporting Context:\n",
        "{retrieval_context}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "actual_output = chain.invoke({\"input\": query, \"retrieval_context\": retrieval_context})\n",
        "print(actual_output.content)\n",
        "expected_output = \"a tensor in deep learning is a fundamental data structure that is a generalization of vectors and matrices to an arbitrary number of dimensions\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdfCrQnV5pbn",
        "outputId": "689f13b1-3671-430a-a61f-ca8077c067ce"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['17\\nintermediate representations, and as output. This chapter is devoted to providing pre -\\ncisely that understanding.\\n T o  t h i s  e n d ,  P y T o r c h  i n t r o d u c e s  a  f u n dam ental data structure: the tensor. For  \\nthose who come from mathematics, ph ysics, or engineering, the term tensor  c o m e s \\nbundled with the notion of spaces, refe rence systems, and transformations between  \\nthem. For everyone else, tensor  refers to the generalization  of vectors and matrices to  \\nan arbitrary number of dimensions, as sh own in figure 2.2. Another name for the  \\nsame concept is multidimensional arrays . The dimensionality of a tensor coincides with  \\nthe number of indexes used to refer to scalar values within the tensor. \\nFigure 2.2 Tensors are the building blocks for representing data in PyTorch\\n PyTorch isn’t not the only library that deals with multidimen sional arrays. NumPy  \\nis by far the most popular mu ltidimensional-array library, to the point that it has argu -']\n",
            "According to the supporting context, a tensor in deep learning is the generalization of vectors and matrices to an arbitrary number of dimensions, also known as multidimensional arrays. The dimensionality of a tensor is the number of indexes used to refer to scalar values within the tensor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "    input=\"What is tensor in deep learning?\",\n",
        "    actual_output = actual_output,\n",
        "    expected_output = expected_output,\n",
        "    retrieval_context = retrieval_context\n",
        ")"
      ],
      "metadata": {
        "id": "zsprVDMgQX41"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OPEN AI: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40vwkKTRVq2S",
        "outputId": "b93aae46-0c80-499f-984a-94e2480c335d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for OPEN AI: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Retrieval"
      ],
      "metadata": {
        "id": "FQkbR6JNX797"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import (\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric,\n",
        "    ContextualRelevancyMetric\n",
        ")\n",
        "\n",
        "contextual_precision = ContextualPrecisionMetric()\n",
        "contextual_recall = ContextualRecallMetric()\n",
        "contextual_relevancy = ContextualRelevancyMetric()\n",
        "\n",
        "contextual_precision.measure(test_case)\n",
        "print(\"Score: \", contextual_precision.score)\n",
        "print(\"Reason: \", contextual_precision.reason)\n",
        "\n",
        "contextual_recall.measure(test_case)\n",
        "print(\"Score: \", contextual_recall.score)\n",
        "print(\"Reason: \", contextual_recall.reason)\n",
        "\n",
        "contextual_relevancy.measure(test_case)\n",
        "print(\"Score: \", contextual_relevancy.score)\n",
        "print(\"Reason: \", contextual_relevancy.reason)"
      ],
      "metadata": {
        "id": "VJXC3cH_R2gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Generation"
      ],
      "metadata": {
        "id": "PWdYPIA5Xzra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
        "\n",
        "answer_relevancy = AnswerRelevancyMetric()\n",
        "faithfulness = FaithfulnessMetric()\n",
        "\n",
        "answer_relevancy.measure(test_case)\n",
        "print(\"Score: \", answer_relevancy.score)\n",
        "print(\"Reason: \", answer_relevancy.reason)\n",
        "\n",
        "faithfulness.measure(test_case)\n",
        "print(\"Score: \", faithfulness.score)\n",
        "print(\"Reason: \", faithfulness.reason)"
      ],
      "metadata": {
        "id": "qR08Ct2lUDO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}