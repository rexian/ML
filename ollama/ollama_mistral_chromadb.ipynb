{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPT9JCOmNho1WEbKtotuQj5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rexian/ML/blob/main/ollama/ollama_mistral_chromadb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install virtualenv\n",
        "!mkdir -p deepeval\n",
        "!cd deepeval\n",
        "!virtualenv venv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS_SCTrk_CTv",
        "outputId": "dbbe165a-79ed-44fd-969f-aded2b74bc1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.11/dist-packages (20.31.2)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (0.3.9)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.18.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.7)\n",
            "created virtual environment CPython3.11.12.final.0-64 in 643ms\n",
            "  creator CPython3Posix(dest=/content/venv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==25.1.1, setuptools==80.3.1\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install DeepEval"
      ],
      "metadata": {
        "id": "5tgf60fP86Gz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXobnuEt6KTQ",
        "outputId": "8450c577-e07a-43aa-bbe6-17b171977555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/680.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m450.6/680.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m680.4/680.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 install -U -q deepeval\n",
        "!pip3 install -U -q chromadb\n",
        "!pip3 install -U -q aiobotocore\n",
        "!pip3 install -U -q aiogram openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!deepeval login --confident-api-key API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEPv81GC9vQY",
        "outputId": "a62b2438-d00d-4b5c-d320-73080138b27b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
            "You're now using DeepEval with \u001b[35mConfident AI\u001b[0m. Follow our quickstart tutorial \n",
            "here: \u001b]8;id=392692;https://documentation.confident-ai.com/getting-started/installation\u001b\\\u001b[1;4;94mhttps://documentation.confident-ai.com/getting-started/installation\u001b[0m\u001b]8;;\u001b\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Environment\n",
        "### Installing and serving Ollama\n",
        "#### - Launching Xterm\n",
        "%xterm\n",
        "#### - Installing Ollama\n",
        "curl https://ollama.ai/install.sh | sh\n",
        "#### - Serving ollama\n",
        "ollama serve &"
      ],
      "metadata": {
        "id": "5gtY1ygmWZmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install colab-xterm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6sgVUJiOBqn",
        "outputId": "b45b87ae-1e2a-4566-9650-6f39c17bda60"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colab-xterm in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.11/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.11/dist-packages (from colab-xterm) (6.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext colabxterm\n",
        "%xterm"
      ],
      "metadata": {
        "id": "AKy_wH1lOM4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull mistral"
      ],
      "metadata": {
        "id": "6Pv8LtUFQDKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbmf7sfsQ_Qk",
        "outputId": "5b3d463e-20f1-4438-f0fe-48387973435e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME              ID              SIZE      MODIFIED      \n",
            "mistral:latest    f974a74358d6    4.1 GB    8 seconds ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U -q langchain-ollama"
      ],
      "metadata": {
        "id": "-z3319SVRLjD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = OllamaLLM(model=\"mistral\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "chain.invoke({\"question\": \"What is hallucination in AI?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "Tbpc0cPiRS2n",
        "outputId": "c4afbf1d-891c-4f02-8ecb-e25ab8b949c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" In the context of AI, a hallucination does not have the same meaning as it does in human psychology. Instead, when we talk about hallucinations in AI, we often refer to situations where an AI interprets or generates data that is not actually present in the input. This could be due to the AI's programming or learning from biased or incorrect data. It's important to note that this is usually undesirable as it can lead to incorrect predictions or decisions by the AI system. To minimize hallucinations, AI models are typically trained on large and diverse datasets and regularly evaluated for accuracy and consistency.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "# Initialize Chroma client\n",
        "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# Create or load a collection\n",
        "collection = client.get_or_create_collection(name=\"rag_documents\")"
      ],
      "metadata": {
        "id": "vykdBfpahgU-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an embedding model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Example document chunks\n",
        "document_chunks = [\n",
        "    \"Chroma is an open-source vector database for efficient embedding retrieval.\",\n",
        "    \"It enables fast semantic search using vector similarity.\",\n",
        "    \"Chroma retrieves relevant data with cosine similarity.\",\n",
        "]\n",
        "\n",
        "# Store chunks with embeddings in Chroma\n",
        "for i, chunk in enumerate(document_chunks):\n",
        "    embedding = embedding_model.encode(chunk).tolist()  # Convert text to vector\n",
        "    collection.add(\n",
        "        ids=[str(i)],  # Unique ID for each document\n",
        "        embeddings=[embedding],  # Vector representation\n",
        "        metadatas=[{\"text\": chunk}]  # Store original text as metadata\n",
        "    )"
      ],
      "metadata": {
        "id": "Q5yCPNCvhnz7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query):\n",
        "  query_embedding = embedding_model.encode(query).tolist()\n",
        "\n",
        "  res = collection.query(\n",
        "    query_embeddings=[query_embedding],\n",
        "    n_results=3  # Retrieve top-K matches\n",
        "  )\n",
        "  result = []\n",
        "  for val in res[\"metadatas\"][0]:\n",
        "    result.append(val['text'])\n",
        "  return  result if result else None\n",
        "\n",
        "query = \"How does Chroma work?\"\n",
        "retrieval_context = search(query)\n",
        "print(retrieval_context)\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Answer the user question based on the supporting context.\n",
        "\n",
        "User Question:\n",
        "{input}\n",
        "\n",
        "Supporting Context:\n",
        "{retrieval_context}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "model = OllamaLLM(model=\"mistral\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "actual_output = chain.invoke({\"input\": query, \"retrieval_context\": retrieval_context})\n",
        "print(actual_output)\n",
        "\n",
        "#expected_output = \"Chroma works by retrieving data that is semantically similar to a given input using a method called cosine similarity. This approach allows it to find and return the most relevant information from its indexed dataset based on the cosine angle between the vector representation of the input query and the vectors of the documents in the dataset.\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8No4NZ8iUVs",
        "outputId": "2764b27d-282c-4357-9a4d-3ca8560dcd0e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chroma retrieves relevant data with cosine similarity.', 'Chroma is an open-source vector database for efficient embedding retrieval.', 'It enables fast semantic search using vector similarity.']\n",
            " Chroma works as an open-source vector database designed for efficient embedding retrieval. It uses a technique called cosine similarity to find and retrieve relevant data that are semantically similar to the query. This allows for fast and accurate semantic search based on vector similarity.\n"
          ]
        }
      ]
    }
  ]
}