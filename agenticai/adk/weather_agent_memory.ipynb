{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMxHK9mTtjy5ng7eeX6KZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rexian/ML/blob/main/agenticai/adk/weather_agent_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onykJQtqgDbg"
      },
      "outputs": [],
      "source": [
        "!pip install google-adk -q\n",
        "!pip install litellm -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Import necessary libraries\n",
        "import os\n",
        "import getpass\n",
        "import asyncio\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "\n",
        "import warnings\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "print(\"Libraries imported.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-elTHKTuzUvm",
        "outputId": "e47050ab-172c-40bc-e1be-d804c98c4805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configure API Keys (Replace with your actual keys!)\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter Google API key: \")\n",
        "os.environ['GROQ_API_KEY'] = getpass.getpass(\"Enter Groq API key: \")\n",
        "\n",
        "print(\"API Keys Set\")\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCppo1vHzmmp",
        "outputId": "8a38630d-bc7a-446c-a07f-35ec40057777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Google API key: ··········\n",
            "Enter Groq API key: ··········\n",
            "API Keys Set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\n",
        "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
        "LLAMA3_8_B = \"groq/llama3-8b-8192\"\n",
        "\n",
        "print(\"\\nEnvironment configured.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7bTbVGX2Ewx",
        "outputId": "06bd4ee4-5c75-4872-801f-e2de677b3af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Environment configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define the get_weather Tool\n",
        "def get_weather(city: str) -> dict:\n",
        "    \"\"\"Retrieves the current weather report for a specified city.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the weather information.\n",
        "              Includes a 'status' key ('success' or 'error').\n",
        "              If 'success', includes a 'report' key with weather details.\n",
        "              If 'error', includes an 'error_message' key.\n",
        "    \"\"\"\n",
        "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
        "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
        "\n",
        "    # Mock weather data\n",
        "    mock_weather_db = {\n",
        "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\n",
        "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15°C.\"},\n",
        "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\n",
        "    }\n",
        "\n",
        "    if city_normalized in mock_weather_db:\n",
        "        return mock_weather_db[city_normalized]\n",
        "    else:\n",
        "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
        "\n",
        "# Example tool usage (optional test)\n",
        "print(get_weather(\"New York\"))\n",
        "print(get_weather(\"Paris\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t-RChEh2Thg",
        "outputId": "6f74dcac-38be-44bb-f4b6-2d5401ca927e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: New York ---\n",
            "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25°C.'}\n",
            "--- Tool: get_weather called for city: Paris ---\n",
            "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define the Weather Agent\n",
        "# Use one of the model constants defined earlier\n",
        "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"weather_agent_v1\",\n",
        "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
        "    description=\"Provides weather information for specific cities.\",\n",
        "    instruction=\"You are a helpful weather assistant. \"\n",
        "                \"When the user asks for the weather in a specific city, \"\n",
        "                \"use the 'get_weather' tool to find the information. \"\n",
        "                \"If the tool returns an error, inform the user politely. \"\n",
        "                \"If the tool is successful, present the weather report clearly.\",\n",
        "    tools=[get_weather], # Pass the function directly\n",
        ")\n",
        "\n",
        "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA5IhkTl2X30",
        "outputId": "c39e27ff-e0fd-4b71-f43d-ddaff73cf5c4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Initialize New Session Service and State\n",
        "\n",
        "# Import necessary session components\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "\n",
        "# Create a NEW session service instance for this state demonstration\n",
        "session_service_stateful = InMemorySessionService()\n",
        "print(\"✅ New InMemorySessionService created for state demonstration.\")\n",
        "\n",
        "# Define a NEW session ID for this part of the tutorial\n",
        "SESSION_ID_STATEFUL = \"session_state_demo_001\"\n",
        "USER_ID_STATEFUL = \"user_state_demo\"\n",
        "APP_NAME = \"weather_agent\"\n",
        "\n",
        "# Define initial state data - user prefers Celsius initially\n",
        "initial_state = {\n",
        "    \"user_preference_temperature_unit\": \"Celsius\"\n",
        "}\n",
        "\n",
        "# Create the session, providing the initial state\n",
        "session_stateful = await session_service_stateful.create_session(\n",
        "    app_name=APP_NAME, # Use the consistent app name\n",
        "    user_id=USER_ID_STATEFUL,\n",
        "    session_id=SESSION_ID_STATEFUL,\n",
        "    state=initial_state # <<< Initialize state during creation\n",
        ")\n",
        "print(f\"✅ Session '{SESSION_ID_STATEFUL}' created for user '{USER_ID_STATEFUL}'.\")\n",
        "\n",
        "# Verify the initial state was set correctly\n",
        "retrieved_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
        "                                                         user_id=USER_ID_STATEFUL,\n",
        "                                                         session_id = SESSION_ID_STATEFUL)\n",
        "print(\"\\n--- Initial Session State ---\")\n",
        "if retrieved_session:\n",
        "    print(retrieved_session.state)\n",
        "else:\n",
        "    print(\"Error: Could not retrieve session.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItzLWp7n2i3H",
        "outputId": "051e7fdc-509c-4d44-da06-c2c78d0cc9dd"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ New InMemorySessionService created for state demonstration.\n",
            "✅ Session 'session_state_demo_001' created for user 'user_state_demo'.\n",
            "\n",
            "--- Initial Session State ---\n",
            "{'user_preference_temperature_unit': 'Celsius'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.adk.tools.tool_context import ToolContext\n",
        "\n",
        "def get_weather_stateful(city: str, tool_context: ToolContext) -> dict:\n",
        "    \"\"\"Retrieves weather, converts temp unit based on session state.\"\"\"\n",
        "    print(f\"--- Tool: get_weather_stateful called for {city} ---\")\n",
        "\n",
        "    # --- Read preference from state ---\n",
        "    preferred_unit = tool_context.state.get(\"user_preference_temperature_unit\", \"Celsius\") # Default to Celsius\n",
        "    print(f\"--- Tool: Reading state 'user_preference_temperature_unit': {preferred_unit} ---\")\n",
        "\n",
        "    city_normalized = city.lower().replace(\" \", \"\")\n",
        "\n",
        "    # Mock weather data (always stored in Celsius internally)\n",
        "    mock_weather_db = {\n",
        "        \"newyork\": {\"temp_c\": 25, \"condition\": \"sunny\"},\n",
        "        \"london\": {\"temp_c\": 15, \"condition\": \"cloudy\"},\n",
        "        \"tokyo\": {\"temp_c\": 18, \"condition\": \"light rain\"},\n",
        "    }\n",
        "\n",
        "    if city_normalized in mock_weather_db:\n",
        "        data = mock_weather_db[city_normalized]\n",
        "        temp_c = data[\"temp_c\"]\n",
        "        condition = data[\"condition\"]\n",
        "\n",
        "        # Format temperature based on state preference\n",
        "        if preferred_unit == \"Fahrenheit\":\n",
        "            temp_value = (temp_c * 9/5) + 32 # Calculate Fahrenheit\n",
        "            temp_unit = \"°F\"\n",
        "        else: # Default to Celsius\n",
        "            temp_value = temp_c\n",
        "            temp_unit = \"°C\"\n",
        "\n",
        "        report = f\"The weather in {city.capitalize()} is {condition} with a temperature of {temp_value:.0f}{temp_unit}.\"\n",
        "        result = {\"status\": \"success\", \"report\": report}\n",
        "        print(f\"--- Tool: Generated report in {preferred_unit}. Result: {result} ---\")\n",
        "\n",
        "        # Example of writing back to state (optional for this tool)\n",
        "        tool_context.state[\"last_city_checked_stateful\"] = city\n",
        "        print(f\"--- Tool: Updated state 'last_city_checked_stateful': {city} ---\")\n",
        "\n",
        "        return result\n",
        "    else:\n",
        "        # Handle city not found\n",
        "        error_msg = f\"Sorry, I don't have weather information for '{city}'.\"\n",
        "        print(f\"--- Tool: City '{city}' not found. ---\")\n",
        "        return {\"status\": \"error\", \"error_message\": error_msg}\n",
        "\n",
        "print(\"✅ State-aware 'get_weather_stateful' tool defined.\")"
      ],
      "metadata": {
        "id": "RtE1AKVh2wkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fdb5225-7ec6-4039-9827-62eb4623d686"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ State-aware 'get_weather_stateful' tool defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Redefine Sub-Agents and Update Root Agent with output_key\n",
        "\n",
        "# Ensure necessary imports: Agent, LiteLlm, Runner\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.models.lite_llm import LiteLlm\n",
        "from google.adk.runners import Runner\n",
        "# Ensure tools 'say_hello', 'say_goodbye' are defined (from Step 3)\n",
        "# Ensure model constants MODEL_GPT_4O, MODEL_GEMINI_2_0_FLASH etc. are defined\n",
        "\n",
        "# --- Redefine Greeting Agent (from Step 3) ---\n",
        "greeting_agent = None\n",
        "try:\n",
        "    greeting_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"greeting_agent\",\n",
        "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
        "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n",
        "        tools=[say_hello],\n",
        "    )\n",
        "    print(f\"✅ Agent '{greeting_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Greeting agent. Error: {e}\")\n",
        "\n",
        "# --- Redefine Farewell Agent (from Step 3) ---\n",
        "farewell_agent = None\n",
        "try:\n",
        "    farewell_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"farewell_agent\",\n",
        "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n",
        "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n",
        "        tools=[say_goodbye],\n",
        "    )\n",
        "    print(f\"✅ Agent '{farewell_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Farewell agent. Error: {e}\")\n",
        "\n",
        "# --- Define the Updated Root Agent ---\n",
        "root_agent_stateful = None\n",
        "runner_root_stateful = None # Initialize runner\n",
        "\n",
        "# Check prerequisites before creating the root agent\n",
        "if greeting_agent and farewell_agent and 'get_weather_stateful' in globals():\n",
        "\n",
        "    root_agent_model = MODEL_GEMINI_2_0_FLASH # Choose orchestration model\n",
        "\n",
        "    root_agent_stateful = Agent(\n",
        "        name=\"weather_agent_v4_stateful\", # New version name\n",
        "        model=root_agent_model,\n",
        "        description=\"Main agent: Provides weather (state-aware unit), delegates greetings/farewells, saves report to state.\",\n",
        "        instruction=\"You are the main Weather Agent. Your job is to provide weather using 'get_weather_stateful'. \"\n",
        "                    \"The tool will format the temperature based on user preference stored in state. \"\n",
        "                    \"Delegate simple greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n",
        "                    \"Handle only weather requests, greetings, and farewells.\",\n",
        "        tools=[get_weather_stateful], # Use the state-aware tool\n",
        "        sub_agents=[greeting_agent, farewell_agent], # Include sub-agents\n",
        "        output_key=\"last_weather_report\" # <<< Auto-save agent's final weather response\n",
        "    )\n",
        "    print(f\"✅ Root Agent '{root_agent_stateful.name}' created using stateful tool and output_key.\")\n",
        "\n",
        "    # --- Create Runner for this Root Agent & NEW Session Service ---\n",
        "    runner_root_stateful = Runner(\n",
        "        agent=root_agent_stateful,\n",
        "        app_name=APP_NAME,\n",
        "        session_service=session_service_stateful # Use the NEW stateful session service\n",
        "    )\n",
        "    print(f\"✅ Runner created for stateful root agent '{runner_root_stateful.agent.name}' using stateful session service.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Cannot create stateful root agent. Prerequisites missing.\")\n",
        "    if not greeting_agent: print(\" - greeting_agent definition missing.\")\n",
        "    if not farewell_agent: print(\" - farewell_agent definition missing.\")\n",
        "    if 'get_weather_stateful' not in globals(): print(\" - get_weather_stateful tool missing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPCRxglP20yN",
        "outputId": "445dcb70-964c-4ec8-9ee5-cf487cd11269"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Agent 'greeting_agent' redefined.\n",
            "✅ Agent 'farewell_agent' redefined.\n",
            "✅ Root Agent 'weather_agent_v4_stateful' created using stateful tool and output_key.\n",
            "✅ Runner created for stateful root agent 'weather_agent_v4_stateful' using stateful session service.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Interact to Test State Flow and output_key\n",
        "import asyncio # Ensure asyncio is imported\n",
        "\n",
        "# Ensure the stateful runner (runner_root_stateful) is available from the previous cell\n",
        "# Ensure call_agent_async, USER_ID_STATEFUL, SESSION_ID_STATEFUL, APP_NAME are defined\n",
        "\n",
        "if 'runner_root_stateful' in globals() and runner_root_stateful:\n",
        "    # Define the main async function for the stateful conversation logic.\n",
        "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
        "    async def run_stateful_conversation():\n",
        "        print(\"\\n--- Testing State: Temp Unit Conversion & output_key ---\")\n",
        "\n",
        "        # 1. Check weather (Uses initial state: Celsius)\n",
        "        print(\"--- Turn 1: Requesting weather in London (expect Celsius) ---\")\n",
        "        await call_agent_async(query= \"What's the weather in London?\",\n",
        "                               runner=runner_root_stateful,\n",
        "                               user_id=USER_ID_STATEFUL,\n",
        "                               session_id=SESSION_ID_STATEFUL\n",
        "                              )\n",
        "\n",
        "        # 2. Manually update state preference to Fahrenheit - DIRECTLY MODIFY STORAGE\n",
        "        print(\"\\n--- Manually Updating State: Setting unit to Fahrenheit ---\")\n",
        "        try:\n",
        "            # Access the internal storage directly - THIS IS SPECIFIC TO InMemorySessionService for testing\n",
        "            # NOTE: In production with persistent services (Database, VertexAI), you would\n",
        "            # typically update state via agent actions or specific service APIs if available,\n",
        "            # not by direct manipulation of internal storage.\n",
        "            stored_session = session_service_stateful.sessions[APP_NAME][USER_ID_STATEFUL][SESSION_ID_STATEFUL]\n",
        "            stored_session.state[\"user_preference_temperature_unit\"] = \"Fahrenheit\"\n",
        "            # Optional: You might want to update the timestamp as well if any logic depends on it\n",
        "            # import time\n",
        "            # stored_session.last_update_time = time.time()\n",
        "            print(f\"--- Stored session state updated. Current 'user_preference_temperature_unit': {stored_session.state.get('user_preference_temperature_unit', 'Not Set')} ---\") # Added .get for safety\n",
        "        except KeyError:\n",
        "            print(f\"--- Error: Could not retrieve session '{SESSION_ID_STATEFUL}' from internal storage for user '{USER_ID_STATEFUL}' in app '{APP_NAME}' to update state. Check IDs and if session was created. ---\")\n",
        "        except Exception as e:\n",
        "             print(f\"--- Error updating internal session state: {e} ---\")\n",
        "\n",
        "        # 3. Check weather again (Tool should now use Fahrenheit)\n",
        "        # This will also update 'last_weather_report' via output_key\n",
        "        print(\"\\n--- Turn 2: Requesting weather in New York (expect Fahrenheit) ---\")\n",
        "        await call_agent_async(query= \"Tell me the weather in New York.\",\n",
        "                               runner=runner_root_stateful,\n",
        "                               user_id=USER_ID_STATEFUL,\n",
        "                               session_id=SESSION_ID_STATEFUL\n",
        "                              )\n",
        "\n",
        "        # 4. Test basic delegation (should still work)\n",
        "        # This will update 'last_weather_report' again, overwriting the NY weather report\n",
        "        print(\"\\n--- Turn 3: Sending a greeting ---\")\n",
        "        await call_agent_async(query= \"Hi!\",\n",
        "                               runner=runner_root_stateful,\n",
        "                               user_id=USER_ID_STATEFUL,\n",
        "                               session_id=SESSION_ID_STATEFUL\n",
        "                              )\n",
        "\n",
        "    # --- Execute the `run_stateful_conversation` async function ---\n",
        "    # Choose ONE of the methods below based on your environment.\n",
        "\n",
        "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
        "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
        "    # it means an event loop is already running, so you can directly await the function.\n",
        "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
        "    await run_stateful_conversation()\n",
        "\n",
        "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
        "    # If running this code as a standard Python script from your terminal,\n",
        "    # the script context is synchronous. `asyncio.run()` is needed to\n",
        "    # create and manage an event loop to execute your async function.\n",
        "    # To use this method:\n",
        "    # 1. Comment out the `await run_stateful_conversation()` line above.\n",
        "    # 2. Uncomment the following block:\n",
        "    \"\"\"\n",
        "    import asyncio\n",
        "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
        "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
        "        try:\n",
        "            # This creates an event loop, runs your async function, and closes the loop.\n",
        "            asyncio.run(run_stateful_conversation())\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Inspect final session state after the conversation ---\n",
        "    # This block runs after either execution method completes.\n",
        "    print(\"\\n--- Inspecting Final Session State ---\")\n",
        "    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
        "                                                         user_id= USER_ID_STATEFUL,\n",
        "                                                         session_id=SESSION_ID_STATEFUL)\n",
        "    if final_session:\n",
        "        # Use .get() for safer access to potentially missing keys\n",
        "        print(f\"Final Preference: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\")\n",
        "        print(f\"Final Last Weather Report (from output_key): {final_session.state.get('last_weather_report', 'Not Set')}\")\n",
        "        print(f\"Final Last City Checked (by tool): {final_session.state.get('last_city_checked_stateful', 'Not Set')}\")\n",
        "        # Print full state for detailed view\n",
        "        # print(f\"Full State Dict: {final_session.state}\") # For detailed view\n",
        "    else:\n",
        "        print(\"\\n❌ Error: Could not retrieve final session state.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠️ Skipping state test conversation. Stateful root agent runner ('runner_root_stateful') is not available.\")"
      ],
      "metadata": {
        "id": "oLOpNnLmTZ1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a624b6-c1d5-470f-cd56-c2a7fb1264e7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting execution using 'await' (default for notebooks)...\n",
            "\n",
            "--- Testing State: Temp Unit Conversion & output_key ---\n",
            "--- Turn 1: Requesting weather in London (expect Celsius) ---\n",
            "\n",
            ">>> User Query: What's the weather in London?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather_stateful called for London ---\n",
            "--- Tool: Reading state 'user_preference_temperature_unit': Celsius ---\n",
            "--- Tool: Generated report in Celsius. Result: {'status': 'success', 'report': 'The weather in London is cloudy with a temperature of 15°C.'} ---\n",
            "--- Tool: Updated state 'last_city_checked_stateful': London ---\n",
            "<<< Agent Response: The weather in London is cloudy with a temperature of 15°C.\n",
            "\n",
            "\n",
            "--- Manually Updating State: Setting unit to Fahrenheit ---\n",
            "--- Stored session state updated. Current 'user_preference_temperature_unit': Fahrenheit ---\n",
            "\n",
            "--- Turn 2: Requesting weather in New York (expect Fahrenheit) ---\n",
            "\n",
            ">>> User Query: Tell me the weather in New York.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather_stateful called for New York ---\n",
            "--- Tool: Reading state 'user_preference_temperature_unit': Fahrenheit ---\n",
            "--- Tool: Generated report in Fahrenheit. Result: {'status': 'success', 'report': 'The weather in New york is sunny with a temperature of 77°F.'} ---\n",
            "--- Tool: Updated state 'last_city_checked_stateful': New York ---\n",
            "<<< Agent Response: The weather in New york is sunny with a temperature of 77°F.\n",
            "\n",
            "\n",
            "--- Turn 3: Sending a greeting ---\n",
            "\n",
            ">>> User Query: Hi!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
            "<<< Agent Response: Hello there!\n",
            "\n",
            "\n",
            "--- Inspecting Final Session State ---\n",
            "Final Preference: Fahrenheit\n",
            "Final Last Weather Report (from output_key): Hello there!\n",
            "\n",
            "Final Last City Checked (by tool): New York\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Define the before_model_callback Guardrail\n",
        "\n",
        "# Ensure necessary imports are available\n",
        "from google.adk.agents.callback_context import CallbackContext\n",
        "from google.adk.models.llm_request import LlmRequest\n",
        "from google.adk.models.llm_response import LlmResponse\n",
        "from google.genai import types # For creating response content\n",
        "from typing import Optional\n",
        "\n",
        "def block_keyword_guardrail(\n",
        "    callback_context: CallbackContext, llm_request: LlmRequest\n",
        ") -> Optional[LlmResponse]:\n",
        "    \"\"\"\n",
        "    Inspects the latest user message for 'BLOCK'. If found, blocks the LLM call\n",
        "    and returns a predefined LlmResponse. Otherwise, returns None to proceed.\n",
        "    \"\"\"\n",
        "    agent_name = callback_context.agent_name # Get the name of the agent whose model call is being intercepted\n",
        "    print(f\"--- Callback: block_keyword_guardrail running for agent: {agent_name} ---\")\n",
        "\n",
        "    # Extract the text from the latest user message in the request history\n",
        "    last_user_message_text = \"\"\n",
        "    if llm_request.contents:\n",
        "        # Find the most recent message with role 'user'\n",
        "        for content in reversed(llm_request.contents):\n",
        "            if content.role == 'user' and content.parts:\n",
        "                # Assuming text is in the first part for simplicity\n",
        "                if content.parts[0].text:\n",
        "                    last_user_message_text = content.parts[0].text\n",
        "                    break # Found the last user message text\n",
        "\n",
        "    print(f\"--- Callback: Inspecting last user message: '{last_user_message_text[:100]}...' ---\") # Log first 100 chars\n",
        "\n",
        "    # --- Guardrail Logic ---\n",
        "    keyword_to_block = \"BLOCK\"\n",
        "    if keyword_to_block in last_user_message_text.upper(): # Case-insensitive check\n",
        "        print(f\"--- Callback: Found '{keyword_to_block}'. Blocking LLM call! ---\")\n",
        "        # Optionally, set a flag in state to record the block event\n",
        "        callback_context.state[\"guardrail_block_keyword_triggered\"] = True\n",
        "        print(f\"--- Callback: Set state 'guardrail_block_keyword_triggered': True ---\")\n",
        "\n",
        "        # Construct and return an LlmResponse to stop the flow and send this back instead\n",
        "        return LlmResponse(\n",
        "            content=types.Content(\n",
        "                role=\"model\", # Mimic a response from the agent's perspective\n",
        "                parts=[types.Part(text=f\"I cannot process this request because it contains the blocked keyword '{keyword_to_block}'.\")],\n",
        "            )\n",
        "            # Note: You could also set an error_message field here if needed\n",
        "        )\n",
        "    else:\n",
        "        # Keyword not found, allow the request to proceed to the LLM\n",
        "        print(f\"--- Callback: Keyword not found. Allowing LLM call for {agent_name}. ---\")\n",
        "        return None # Returning None signals ADK to continue normally\n",
        "\n",
        "print(\"✅ block_keyword_guardrail function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCB5ABY6UBw2",
        "outputId": "e95eacfa-3e38-4653-84c6-031b6fecb316"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ block_keyword_guardrail function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Update Root Agent with before_model_callback\n",
        "\n",
        "\n",
        "# --- Redefine Sub-Agents (Ensures they exist in this context) ---\n",
        "greeting_agent = None\n",
        "try:\n",
        "    # Use a defined model constant\n",
        "    greeting_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"greeting_agent\", # Keep original name for consistency\n",
        "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
        "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n",
        "        tools=[say_hello],\n",
        "    )\n",
        "    print(f\"✅ Sub-Agent '{greeting_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Greeting agent. Check Model/API Key ({greeting_agent.model}). Error: {e}\")\n",
        "\n",
        "farewell_agent = None\n",
        "try:\n",
        "    # Use a defined model constant\n",
        "    farewell_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"farewell_agent\", # Keep original name\n",
        "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n",
        "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n",
        "        tools=[say_goodbye],\n",
        "    )\n",
        "    print(f\"✅ Sub-Agent '{farewell_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Farewell agent. Check Model/API Key ({farewell_agent.model}). Error: {e}\")\n",
        "\n",
        "\n",
        "# --- Define the Root Agent with the Callback ---\n",
        "root_agent_model_guardrail = None\n",
        "runner_root_model_guardrail = None\n",
        "\n",
        "# Check all components before proceeding\n",
        "if greeting_agent and farewell_agent and 'get_weather_stateful' in globals() and 'block_keyword_guardrail' in globals():\n",
        "\n",
        "    # Use a defined model constant\n",
        "    root_agent_model = MODEL_GEMINI_2_0_FLASH\n",
        "\n",
        "    root_agent_model_guardrail = Agent(\n",
        "        name=\"weather_agent_v5_model_guardrail\", # New version name for clarity\n",
        "        model=root_agent_model,\n",
        "        description=\"Main agent: Handles weather, delegates greetings/farewells, includes input keyword guardrail.\",\n",
        "        instruction=\"You are the main Weather Agent. Provide weather using 'get_weather_stateful'. \"\n",
        "                    \"Delegate simple greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n",
        "                    \"Handle only weather requests, greetings, and farewells.\",\n",
        "        tools=[get_weather],\n",
        "        sub_agents=[greeting_agent, farewell_agent], # Reference the redefined sub-agents\n",
        "        output_key=\"last_weather_report\", # Keep output_key from Step 4\n",
        "        before_model_callback=block_keyword_guardrail # <<< Assign the guardrail callback\n",
        "    )\n",
        "    print(f\"✅ Root Agent '{root_agent_model_guardrail.name}' created with before_model_callback.\")\n",
        "\n",
        "    # --- Create Runner for this Agent, Using SAME Stateful Session Service ---\n",
        "    # Ensure session_service_stateful exists from Step 4\n",
        "    if 'session_service_stateful' in globals():\n",
        "        runner_root_model_guardrail = Runner(\n",
        "            agent=root_agent_model_guardrail,\n",
        "            app_name=APP_NAME, # Use consistent APP_NAME\n",
        "            session_service=session_service_stateful # <<< Use the service from Step 4\n",
        "        )\n",
        "        print(f\"✅ Runner created for guardrail agent '{runner_root_model_guardrail.agent.name}', using stateful session service.\")\n",
        "    else:\n",
        "        print(\"❌ Cannot create runner. 'session_service_stateful' from Step 4 is missing.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Cannot create root agent with model guardrail. One or more prerequisites are missing or failed initialization:\")\n",
        "    if not greeting_agent: print(\"   - Greeting Agent\")\n",
        "    if not farewell_agent: print(\"   - Farewell Agent\")\n",
        "    if 'get_weather_stateful' not in globals(): print(\"   - 'get_weather_stateful' tool\")\n",
        "    if 'block_keyword_guardrail' not in globals(): print(\"   - 'block_keyword_guardrail' callback\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8nDOm49Zex1",
        "outputId": "acd4740f-6eec-484e-923f-ab098d213cd4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sub-Agent 'greeting_agent' redefined.\n",
            "✅ Sub-Agent 'farewell_agent' redefined.\n",
            "✅ Root Agent 'weather_agent_v5_model_guardrail' created with before_model_callback.\n",
            "✅ Runner created for guardrail agent 'weather_agent_v5_model_guardrail', using stateful session service.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Interact to Test the Model Input Guardrail\n",
        "import asyncio # Ensure asyncio is imported\n",
        "\n",
        "# Ensure the runner for the guardrail agent is available\n",
        "if 'runner_root_model_guardrail' in globals() and runner_root_model_guardrail:\n",
        "    # Define the main async function for the guardrail test conversation.\n",
        "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
        "    async def run_guardrail_test_conversation():\n",
        "        print(\"\\n--- Testing Model Input Guardrail ---\")\n",
        "\n",
        "        # Use the runner for the agent with the callback and the existing stateful session ID\n",
        "        # Define a helper lambda for cleaner interaction calls\n",
        "        interaction_func = lambda query: call_agent_async(query,\n",
        "                                                         runner_root_model_guardrail,\n",
        "                                                         USER_ID_STATEFUL, # Use existing user ID\n",
        "                                                         SESSION_ID_STATEFUL # Use existing session ID\n",
        "                                                        )\n",
        "        # 1. Normal request (Callback allows, should use Fahrenheit from previous state change)\n",
        "        print(\"--- Turn 1: Requesting weather in London (expect allowed, Fahrenheit) ---\")\n",
        "        await interaction_func(\"What is the weather in London?\")\n",
        "\n",
        "        # 2. Request containing the blocked keyword (Callback intercepts)\n",
        "        print(\"\\n--- Turn 2: Requesting with blocked keyword (expect blocked) ---\")\n",
        "        await interaction_func(\"BLOCK the request for weather in Tokyo\") # Callback should catch \"BLOCK\"\n",
        "\n",
        "        # 3. Normal greeting (Callback allows root agent, delegation happens)\n",
        "        print(\"\\n--- Turn 3: Sending a greeting (expect allowed) ---\")\n",
        "        await interaction_func(\"Hello again\")\n",
        "\n",
        "    # --- Execute the `run_guardrail_test_conversation` async function ---\n",
        "    # Choose ONE of the methods below based on your environment.\n",
        "\n",
        "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
        "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
        "    # it means an event loop is already running, so you can directly await the function.\n",
        "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
        "    await run_guardrail_test_conversation()\n",
        "\n",
        "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
        "    # If running this code as a standard Python script from your terminal,\n",
        "    # the script context is synchronous. `asyncio.run()` is needed to\n",
        "    # create and manage an event loop to execute your async function.\n",
        "    # To use this method:\n",
        "    # 1. Comment out the `await run_guardrail_test_conversation()` line above.\n",
        "    # 2. Uncomment the following block:\n",
        "    \"\"\"\n",
        "    import asyncio\n",
        "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
        "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
        "        try:\n",
        "            # This creates an event loop, runs your async function, and closes the loop.\n",
        "            asyncio.run(run_guardrail_test_conversation())\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Inspect final session state after the conversation ---\n",
        "    # This block runs after either execution method completes.\n",
        "    # Optional: Check state for the trigger flag set by the callback\n",
        "    print(\"\\n--- Inspecting Final Session State (After Guardrail Test) ---\")\n",
        "    # Use the session service instance associated with this stateful session\n",
        "    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
        "                                                         user_id=USER_ID_STATEFUL,\n",
        "                                                         session_id=SESSION_ID_STATEFUL)\n",
        "    if final_session:\n",
        "        # Use .get() for safer access\n",
        "        print(f\"Guardrail Triggered Flag: {final_session.state.get('guardrail_block_keyword_triggered', 'Not Set (or False)')}\")\n",
        "        print(f\"Last Weather Report: {final_session.state.get('last_weather_report', 'Not Set')}\") # Should be London weather if successful\n",
        "        print(f\"Temperature Unit: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\") # Should be Fahrenheit\n",
        "        # print(f\"Full State Dict: {final_session.state}\") # For detailed view\n",
        "    else:\n",
        "        print(\"\\n❌ Error: Could not retrieve final session state.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠️ Skipping model guardrail test. Runner ('runner_root_model_guardrail') is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJj_0k7sZslw",
        "outputId": "17a5aef9-5c23-4ae5-c540-9d38c74615c6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting execution using 'await' (default for notebooks)...\n",
            "\n",
            "--- Testing Model Input Guardrail ---\n",
            "--- Turn 1: Requesting weather in London (expect allowed, Fahrenheit) ---\n",
            "\n",
            ">>> User Query: What is the weather in London?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'For context:...' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v5_model_guardrail. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: London ---\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'For context:...' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v5_model_guardrail. ---\n",
            "<<< Agent Response: It's cloudy in London with a temperature of 15°C.\n",
            "\n",
            "\n",
            "--- Turn 2: Requesting with blocked keyword (expect blocked) ---\n",
            "\n",
            ">>> User Query: BLOCK the request for weather in Tokyo\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'BLOCK the request for weather in Tokyo...' ---\n",
            "--- Callback: Found 'BLOCK'. Blocking LLM call! ---\n",
            "--- Callback: Set state 'guardrail_block_keyword_triggered': True ---\n",
            "<<< Agent Response: I cannot process this request because it contains the blocked keyword 'BLOCK'.\n",
            "\n",
            "--- Turn 3: Sending a greeting (expect allowed) ---\n",
            "\n",
            ">>> User Query: Hello again\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v5_model_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'Hello again...' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v5_model_guardrail. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
            "<<< Agent Response: Hello there!\n",
            "\n",
            "\n",
            "--- Inspecting Final Session State (After Guardrail Test) ---\n",
            "Guardrail Triggered Flag: True\n",
            "Last Weather Report: Hello there!\n",
            "\n",
            "Temperature Unit: Fahrenheit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Define the before_tool_callback Guardrail\n",
        "\n",
        "# Ensure necessary imports are available\n",
        "from google.adk.tools.base_tool import BaseTool\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "from typing import Optional, Dict, Any # For type hints\n",
        "\n",
        "def block_paris_tool_guardrail(\n",
        "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Checks if 'get_weather_stateful' is called for 'Paris'.\n",
        "    If so, blocks the tool execution and returns a specific error dictionary.\n",
        "    Otherwise, allows the tool call to proceed by returning None.\n",
        "    \"\"\"\n",
        "    tool_name = tool.name\n",
        "    agent_name = tool_context.agent_name # Agent attempting the tool call\n",
        "    print(f\"--- Callback: block_paris_tool_guardrail running for tool '{tool_name}' in agent '{agent_name}' ---\")\n",
        "    print(f\"--- Callback: Inspecting args: {args} ---\")\n",
        "\n",
        "    # --- Guardrail Logic ---\n",
        "    target_tool_name = \"get_weather_stateful\" # Match the function name used by FunctionTool\n",
        "    blocked_city = \"paris\"\n",
        "\n",
        "    # Check if it's the correct tool and the city argument matches the blocked city\n",
        "    if tool_name == target_tool_name:\n",
        "        city_argument = args.get(\"city\", \"\") # Safely get the 'city' argument\n",
        "        if city_argument and city_argument.lower() == blocked_city:\n",
        "            print(f\"--- Callback: Detected blocked city '{city_argument}'. Blocking tool execution! ---\")\n",
        "            # Optionally update state\n",
        "            tool_context.state[\"guardrail_tool_block_triggered\"] = True\n",
        "            print(f\"--- Callback: Set state 'guardrail_tool_block_triggered': True ---\")\n",
        "\n",
        "            # Return a dictionary matching the tool's expected output format for errors\n",
        "            # This dictionary becomes the tool's result, skipping the actual tool run.\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"error_message\": f\"Policy restriction: Weather checks for '{city_argument.capitalize()}' are currently disabled by a tool guardrail.\"\n",
        "            }\n",
        "        else:\n",
        "             print(f\"--- Callback: City '{city_argument}' is allowed for tool '{tool_name}'. ---\")\n",
        "    else:\n",
        "        print(f\"--- Callback: Tool '{tool_name}' is not the target tool. Allowing. ---\")\n",
        "\n",
        "\n",
        "    # If the checks above didn't return a dictionary, allow the tool to execute\n",
        "    print(f\"--- Callback: Allowing tool '{tool_name}' to proceed. ---\")\n",
        "    return None # Returning None allows the actual tool function to run\n",
        "\n",
        "print(\"✅ block_paris_tool_guardrail function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EggNMKdZ3My",
        "outputId": "02162fa2-7b4d-4cc1-8899-9216b7602de4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ block_paris_tool_guardrail function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Update Root Agent with BOTH Callbacks (Self-Contained)\n",
        "\n",
        "# --- Ensure Prerequisites are Defined ---\n",
        "# (Include or ensure execution of definitions for: Agent, LiteLlm, Runner, ToolContext,\n",
        "#  MODEL constants, say_hello, say_goodbye, greeting_agent, farewell_agent,\n",
        "#  get_weather_stateful, block_keyword_guardrail, block_paris_tool_guardrail)\n",
        "\n",
        "# --- Redefine Sub-Agents (Ensures they exist in this context) ---\n",
        "greeting_agent = None\n",
        "try:\n",
        "    # Use a defined model constant\n",
        "    greeting_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"greeting_agent\", # Keep original name for consistency\n",
        "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
        "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n",
        "        tools=[say_hello],\n",
        "    )\n",
        "    print(f\"✅ Sub-Agent '{greeting_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Greeting agent. Check Model/API Key ({greeting_agent.model}). Error: {e}\")\n",
        "\n",
        "farewell_agent = None\n",
        "try:\n",
        "    # Use a defined model constant\n",
        "    farewell_agent = Agent(\n",
        "        model=MODEL_GEMINI_2_0_FLASH,\n",
        "        name=\"farewell_agent\", # Keep original name\n",
        "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n",
        "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n",
        "        tools=[say_goodbye],\n",
        "    )\n",
        "    print(f\"✅ Sub-Agent '{farewell_agent.name}' redefined.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Could not redefine Farewell agent. Check Model/API Key ({farewell_agent.model}). Error: {e}\")\n",
        "\n",
        "# --- Define the Root Agent with Both Callbacks ---\n",
        "root_agent_tool_guardrail = None\n",
        "runner_root_tool_guardrail = None\n",
        "\n",
        "if ('greeting_agent' in globals() and greeting_agent and\n",
        "    'farewell_agent' in globals() and farewell_agent and\n",
        "    'get_weather_stateful' in globals() and\n",
        "    'block_keyword_guardrail' in globals() and\n",
        "    'block_paris_tool_guardrail' in globals()):\n",
        "\n",
        "    root_agent_model = MODEL_GEMINI_2_0_FLASH\n",
        "\n",
        "    root_agent_tool_guardrail = Agent(\n",
        "        name=\"weather_agent_v6_tool_guardrail\", # New version name\n",
        "        model=root_agent_model,\n",
        "        description=\"Main agent: Handles weather, delegates, includes input AND tool guardrails.\",\n",
        "        instruction=\"You are the main Weather Agent. Provide weather using 'get_weather_stateful'. \"\n",
        "                    \"Delegate greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n",
        "                    \"Handle only weather, greetings, and farewells.\",\n",
        "        tools=[get_weather_stateful],\n",
        "        sub_agents=[greeting_agent, farewell_agent],\n",
        "        output_key=\"last_weather_report\",\n",
        "        before_model_callback=block_keyword_guardrail, # Keep model guardrail\n",
        "        before_tool_callback=block_paris_tool_guardrail # <<< Add tool guardrail\n",
        "    )\n",
        "    print(f\"✅ Root Agent '{root_agent_tool_guardrail.name}' created with BOTH callbacks.\")\n",
        "\n",
        "    # --- Create Runner, Using SAME Stateful Session Service ---\n",
        "    if 'session_service_stateful' in globals():\n",
        "        runner_root_tool_guardrail = Runner(\n",
        "            agent=root_agent_tool_guardrail,\n",
        "            app_name=APP_NAME,\n",
        "            session_service=session_service_stateful # <<< Use the service from Step 4/5\n",
        "        )\n",
        "        print(f\"✅ Runner created for tool guardrail agent '{runner_root_tool_guardrail.agent.name}', using stateful session service.\")\n",
        "    else:\n",
        "        print(\"❌ Cannot create runner. 'session_service_stateful' from Step 4/5 is missing.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Cannot create root agent with tool guardrail. Prerequisites missing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O42WfD70eV3a",
        "outputId": "8f14edfc-906c-43ba-caa0-0c7c31c71dc5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sub-Agent 'greeting_agent' redefined.\n",
            "✅ Sub-Agent 'farewell_agent' redefined.\n",
            "✅ Root Agent 'weather_agent_v6_tool_guardrail' created with BOTH callbacks.\n",
            "✅ Runner created for tool guardrail agent 'weather_agent_v6_tool_guardrail', using stateful session service.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Interact to Test the Tool Argument Guardrail\n",
        "import asyncio # Ensure asyncio is imported\n",
        "\n",
        "# Ensure the runner for the tool guardrail agent is available\n",
        "if 'runner_root_tool_guardrail' in globals() and runner_root_tool_guardrail:\n",
        "    # Define the main async function for the tool guardrail test conversation.\n",
        "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
        "    async def run_tool_guardrail_test():\n",
        "        print(\"\\n--- Testing Tool Argument Guardrail ('Paris' blocked) ---\")\n",
        "\n",
        "        # Use the runner for the agent with both callbacks and the existing stateful session\n",
        "        # Define a helper lambda for cleaner interaction calls\n",
        "        interaction_func = lambda query: call_agent_async(query,\n",
        "                                                         runner_root_tool_guardrail,\n",
        "                                                         USER_ID_STATEFUL, # Use existing user ID\n",
        "                                                         SESSION_ID_STATEFUL # Use existing session ID\n",
        "                                                        )\n",
        "        # 1. Allowed city (Should pass both callbacks, use Fahrenheit state)\n",
        "        print(\"--- Turn 1: Requesting weather in New York (expect allowed) ---\")\n",
        "        await interaction_func(\"What's the weather in New York?\")\n",
        "\n",
        "        # 2. Blocked city (Should pass model callback, but be blocked by tool callback)\n",
        "        print(\"\\n--- Turn 2: Requesting weather in Paris (expect blocked by tool guardrail) ---\")\n",
        "        await interaction_func(\"How about Paris?\") # Tool callback should intercept this\n",
        "\n",
        "        # 3. Another allowed city (Should work normally again)\n",
        "        print(\"\\n--- Turn 3: Requesting weather in London (expect allowed) ---\")\n",
        "        await interaction_func(\"Tell me the weather in London.\")\n",
        "\n",
        "    # --- Execute the `run_tool_guardrail_test` async function ---\n",
        "    # Choose ONE of the methods below based on your environment.\n",
        "\n",
        "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
        "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
        "    # it means an event loop is already running, so you can directly await the function.\n",
        "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
        "    await run_tool_guardrail_test()\n",
        "\n",
        "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
        "    # If running this code as a standard Python script from your terminal,\n",
        "    # the script context is synchronous. `asyncio.run()` is needed to\n",
        "    # create and manage an event loop to execute your async function.\n",
        "    # To use this method:\n",
        "    # 1. Comment out the `await run_tool_guardrail_test()` line above.\n",
        "    # 2. Uncomment the following block:\n",
        "    \"\"\"\n",
        "    import asyncio\n",
        "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
        "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
        "        try:\n",
        "            # This creates an event loop, runs your async function, and closes the loop.\n",
        "            asyncio.run(run_tool_guardrail_test())\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Inspect final session state after the conversation ---\n",
        "    # This block runs after either execution method completes.\n",
        "    # Optional: Check state for the tool block trigger flag\n",
        "    print(\"\\n--- Inspecting Final Session State (After Tool Guardrail Test) ---\")\n",
        "    # Use the session service instance associated with this stateful session\n",
        "    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
        "                                                         user_id=USER_ID_STATEFUL,\n",
        "                                                         session_id= SESSION_ID_STATEFUL)\n",
        "    if final_session:\n",
        "        # Use .get() for safer access\n",
        "        print(f\"Tool Guardrail Triggered Flag: {final_session.state.get('guardrail_tool_block_triggered', 'Not Set (or False)')}\")\n",
        "        print(f\"Last Weather Report: {final_session.state.get('last_weather_report', 'Not Set')}\") # Should be London weather if successful\n",
        "        print(f\"Temperature Unit: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\") # Should be Fahrenheit\n",
        "        # print(f\"Full State Dict: {final_session.state}\") # For detailed view\n",
        "    else:\n",
        "        print(\"\\n❌ Error: Could not retrieve final session state.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠️ Skipping tool guardrail test. Runner ('runner_root_tool_guardrail') is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iva1qLnCecMY",
        "outputId": "d83b055c-de57-4265-a6e7-d762fbea52f0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting execution using 'await' (default for notebooks)...\n",
            "\n",
            "--- Testing Tool Argument Guardrail ('Paris' blocked) ---\n",
            "--- Turn 1: Requesting weather in New York (expect allowed) ---\n",
            "\n",
            ">>> User Query: What's the weather in New York?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'For context:...' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Callback: block_paris_tool_guardrail running for tool 'get_weather_stateful' in agent 'weather_agent_v6_tool_guardrail' ---\n",
            "--- Callback: Inspecting args: {'city': 'New York'} ---\n",
            "--- Callback: City 'New York' is allowed for tool 'get_weather_stateful'. ---\n",
            "--- Callback: Allowing tool 'get_weather_stateful' to proceed. ---\n",
            "--- Tool: get_weather_stateful called for New York ---\n",
            "--- Tool: Reading state 'user_preference_temperature_unit': Fahrenheit ---\n",
            "--- Tool: Generated report in Fahrenheit. Result: {'status': 'success', 'report': 'The weather in New york is sunny with a temperature of 77°F.'} ---\n",
            "--- Tool: Updated state 'last_city_checked_stateful': New York ---\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'For context:...' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
            "<<< Agent Response: The weather in New york is sunny with a temperature of 77°F.\n",
            "\n",
            "\n",
            "--- Turn 2: Requesting weather in Paris (expect blocked by tool guardrail) ---\n",
            "\n",
            ">>> User Query: How about Paris?\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'How about Paris?...' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Callback: block_paris_tool_guardrail running for tool 'get_weather_stateful' in agent 'weather_agent_v6_tool_guardrail' ---\n",
            "--- Callback: Inspecting args: {'city': 'Paris'} ---\n",
            "--- Callback: Detected blocked city 'Paris'. Blocking tool execution! ---\n",
            "--- Callback: Set state 'guardrail_tool_block_triggered': True ---\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'How about Paris?...' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
            "<<< Agent Response: I am sorry, weather checks for Paris are currently disabled.\n",
            "\n",
            "\n",
            "--- Turn 3: Requesting weather in London (expect allowed) ---\n",
            "\n",
            ">>> User Query: Tell me the weather in London.\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'Tell me the weather in London....' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Callback: block_paris_tool_guardrail running for tool 'get_weather_stateful' in agent 'weather_agent_v6_tool_guardrail' ---\n",
            "--- Callback: Inspecting args: {'city': 'London'} ---\n",
            "--- Callback: City 'London' is allowed for tool 'get_weather_stateful'. ---\n",
            "--- Callback: Allowing tool 'get_weather_stateful' to proceed. ---\n",
            "--- Tool: get_weather_stateful called for London ---\n",
            "--- Tool: Reading state 'user_preference_temperature_unit': Fahrenheit ---\n",
            "--- Tool: Generated report in Fahrenheit. Result: {'status': 'success', 'report': 'The weather in London is cloudy with a temperature of 59°F.'} ---\n",
            "--- Tool: Updated state 'last_city_checked_stateful': London ---\n",
            "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
            "--- Callback: Inspecting last user message: 'Tell me the weather in London....' ---\n",
            "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
            "<<< Agent Response: The weather in London is cloudy with a temperature of 59°F.\n",
            "\n",
            "\n",
            "--- Inspecting Final Session State (After Tool Guardrail Test) ---\n",
            "Tool Guardrail Triggered Flag: True\n",
            "Last Weather Report: The weather in London is cloudy with a temperature of 59°F.\n",
            "\n",
            "Temperature Unit: Fahrenheit\n"
          ]
        }
      ]
    }
  ]
}