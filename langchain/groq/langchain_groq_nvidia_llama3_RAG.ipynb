{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPqn+4P7cwhMVnKxQ1QZj/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rexian/ML/blob/main/langchain/groq/langchain_groq_nvidia_llama3_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT8tEuvV3dR2"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-core langgraph>0.2.27\n",
        "!pip install -qU \"langchain[groq]\" langchain_community langchain_huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRuzwtZZODaA",
        "outputId": "97053bc9-3bda-43a6-ddfa-49b5535ac276"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Groq: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"golden_hymns_of_epictetus.txt\" https://www.gutenberg.org/cache/epub/871/pg871.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xJOc7DfOPMq",
        "outputId": "1b16c42c-9504-4a57-aef5-945f16d4826e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-26 20:51:02--  https://www.gutenberg.org/cache/epub/871/pg871.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152337 (149K) [text/plain]\n",
            "Saving to: ‘golden_hymns_of_epictetus.txt’\n",
            "\n",
            "golden_hymns_of_epi 100%[===================>] 148.77K   711KB/s    in 0.2s    \n",
            "\n",
            "2025-03-26 20:51:02 (711 KB/s) - ‘golden_hymns_of_epictetus.txt’ saved [152337/152337]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/golden_hymns_of_epictetus.txt\"\n",
        "\n",
        "start_saving = False\n",
        "stop_saving = False\n",
        "lines_to_save = []\n",
        "\n",
        "with open(filename, 'r') as file:\n",
        "    for line in file:\n",
        "        if \"Are these the only works of Providence within us?\" in line:\n",
        "            start_saving = True\n",
        "        if \"*** END OF THE PROJECT GUTENBERG EBOOK THE GOLDEN SAYINGS OF EPICTETUS, WITH THE HYMN OF CLEANTHES ***\" in line:\n",
        "            stop_saving = True\n",
        "            break\n",
        "        if start_saving and not stop_saving:\n",
        "            lines_to_save.append(line)\n",
        "\n",
        "# Write the stored lines back to the file\n",
        "with open(filename, 'w') as file:\n",
        "    for line in lines_to_save:\n",
        "        file.write(line)\n",
        "word_count = 0\n",
        "\n",
        "with open(filename, 'r') as file:\n",
        "    for line in file:\n",
        "        words = line.split()\n",
        "        word_count += len(words)\n",
        "\n",
        "print(f\"The total number of words in the file is: {word_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSWhl79hOVfW",
        "outputId": "af9ecd82-5a93-4f77-95d8-18818bc7d50d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of words in the file is: 23503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval gathers resources to enhance an essay, helping language models access up-to-date, relevant information beyond their built-in knowledge.\n",
        "\n",
        "- **Advantages**:\n",
        "   - Adds new, fresh information.\n",
        "   - Makes responses more relevant and informed.\n",
        "\n",
        "- **Document Loaders**:\n",
        "   - Function as \"specialized librarians.\"\n",
        "   - Organize content from various sources for language models.\n",
        "\n",
        "- **Text Loader Fundamentals**:\n",
        "   - Simple process: Converts text files into a usable format for language models.\n",
        "\n",
        "- **Presentation Style**:\n",
        "   - Brief and informative, ideal for a concise summary."
      ],
      "metadata": {
        "id": "xzOCctaeQOuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader(\"/content/golden_hymns_of_epictetus.txt\")\n",
        "golden_sayings = loader.load()\n",
        "\n",
        "type(golden_sayings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSQ_AQJcOjiG",
        "outputId": "06a5941a-9a04-44cd-f12b-b9a44433f86a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Document Loaders**:\n",
        "\n",
        "**Usage Steps**:\n",
        "   1. Choose a Document Loader from LangChain.\n",
        "   2. Create an instance of the Document Loader.\n",
        "   3. Employ its `load()` method to convert files into LangChain documents.\n",
        "\n",
        "**Role of Document Transformers**\n",
        "\n",
        "Customization for Models: Adjust documents to suit model's requirements, like trimming lengthy texts.\n",
        "\n",
        "**Understanding Text Splitters**\n",
        "\n",
        "**Function**: Divide long texts into smaller, coherent segments.\n",
        "\n",
        "**Goal**: Keep related text together, fitting within the model's capacity.\n",
        "\n",
        "**Using `RecursiveCharacterTextSplitter`**\n",
        "\n",
        "**Methodology**:\n",
        "   - Intelligently splits texts using multiple separators.\n",
        "\n",
        "   - Recursively adjusts if segments are too large.\n",
        "\n",
        "   - Ensures all parts are appropriately sized.\n",
        "\n",
        "**Key Aspects of Splitting**\n",
        "\n",
        "   - Chooses optimal separators for division.\n",
        "\n",
        "   - Continually splits large chunks.\n",
        "\n",
        "   - Balances chunk size by characters or tokens.\n",
        "\n",
        "   - Maintains some overlap for context.\n",
        "\n",
        "   - Tracks chunk starting points if needed.\n",
        "\n",
        "**Presentation Style**\n",
        "\n",
        "   - Focused on essential steps and features, great for a concise summary."
      ],
      "metadata": {
        "id": "baLfErhQQyDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap = 50,\n",
        "    length_function = len,\n",
        "    add_start_index = True\n",
        ")\n",
        "texts = text_splitter.split_documents(golden_sayings)\n",
        "texts[0].page_content\n",
        "texts[1].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "pNi3pWftRXCR",
        "outputId": "ea0cd5e3-1a29-4f75-98d7-d7bccf55dbdc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What then! seeing that most of you are blinded, should there not be\\nsome one to fill this place, and sing the hymn to God on behalf of all\\nmen? What else can I that am old and lame do but sing to God? Were I a\\nnightingale, I should do after the manner of a nightingale. Were I a\\nswan, I should do after the manner of a swan. But now, since I am a\\nreasonable being, I must sing to God: that is my work: I do it, nor\\nwill I desert this my post, as long as it is granted me to hold it; and\\nupon you too I call to join in this self-same hymn.\\n\\nII\\n\\nHow then do men act? As though one returning to his country who had\\nsojourned for the night in a fair inn, should be so captivated thereby\\nas to take up his abode there.\\n\\n“Friend, thou hast forgotten thine intention! This was not thy\\ndestination, but only lay on the way thither.”\\n\\n“Nay, but it is a proper place.”'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Embeddings Overview**\n",
        "\n",
        "**Functionality**: Converts documents into numerical vectors in LangChain.\n",
        "\n",
        "**Similarity Measure**: Vectors that are closer indicate more similar texts.\n",
        "\n",
        "**Application**: Quickly identify documents with similar topics or content.\n",
        "\n",
        "**Presentation Style**: Concise and clear, ideal for slides or quick explanations."
      ],
      "metadata": {
        "id": "_Vbow8rISAxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "k_oZTECGSH1c"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating a Vector Store Retriever**\n",
        "\n",
        "1. **Load Documents**: Utilize a document loader for initial document retrieval.\n",
        "\n",
        "2. **Split Texts**: Break down documents into smaller sections with a text splitter.\n",
        "\n",
        "3. **Embedding Conversion**: Apply an embedding model to transform text chunks into vectors.\n",
        "\n",
        "4. **Vector Store Creation**: Compile these vectors into a vector store.\n",
        "\n",
        "**Outcome**: Your vector store is now set up to search and retrieve texts by content."
      ],
      "metadata": {
        "id": "S1S5Y3EWrsHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu \tlangchain-nvidia-ai-endpoints"
      ],
      "metadata": {
        "id": "kC19Ta7asc_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "vectorstore = FAISS.from_documents(documents=texts, embedding=hf)"
      ],
      "metadata": {
        "id": "vVgBovWArw70"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA AI Endpoints: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDXXs8Fqy7s_",
        "outputId": "2238a742-e33d-4903-cbca-9cf3894fe76f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for NVIDIA AI Endpoints: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "template = \"\"\"\n",
        "\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say 'Ah snap homie, I ain't gonna front. I don't know.`, don't try to make up an answer.\n",
        "Use three sentences maximum, relevant analogies, and keep the answer as concise as possible.\n",
        "Use the active voice, and speak directly to the reader using concise language.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "nvidia_model = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    nvidia_model,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n",
        "\n",
        "query = \"What do grief, fear, envy, and desire stem from?\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2xrJilD-tMNz",
        "outputId": "45c08e0c-c44f-453f-e994-d5a049f70f91"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Homie, grief, fear, envy, and desire stem from not having your own good, your own best interest, in that which is free from hindrance and in your power. Instead, they arise from placing your worth in things that are subject to hindrance and dependent on the will of others.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LCEL for Retrieval**\n",
        "\n",
        "1. **Integrate Context and Question**: The prompt template includes placeholders for context and question.\n",
        "\n",
        "2. **Preliminary Setup**\n",
        "   - Set up a retriever with an in-memory store for document retrieval.\n",
        "   - Runnable components can be chained or run separately.\n",
        "\n",
        "3. **RunnableParallel for Input Preparation**\n",
        "   - Use `RunnableParallel` to combine document search results and the user's question.\n",
        "   - `RunnablePassthrough` passes the user's question unchanged.\n",
        "\n",
        "4. **Workflow Steps**\n",
        "   - **Step 1**: Create `RunnableParallel` with two entries: 'context' (document results) and 'question' (user's original query).\n",
        "   - **Step 2**: Feed the dictionary to the prompt component, which constructs a prompt using the user's question and retrieved documents.\n",
        "   - **Step 3**: Model component evaluates the prompt with Llama3 LLM\n",
        "   - **Step 4**: `Output_parser` transforms response into a readable Python string.\n",
        "\n",
        "**End-to-End Process**: From document retrieval and prompt creation to model evaluation and output parsing, the flow seamlessly integrates various components for an effective LLM-driven response."
      ],
      "metadata": {
        "id": "TN67F_HZ1WjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "setup_and_retrieval = RunnableParallel(\n",
        "    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
        ")\n",
        "chain = setup_and_retrieval | QA_CHAIN_PROMPT | nvidia_model | output_parser\n",
        "\n",
        "chain.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "HjUmdYyS12wP",
        "outputId": "4dcd0521-9bcb-4076-eede-420a275ab307"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the provided context, it seems that grief, fear, envy, and desire stem from one's inability to cope with their own emotions and desires, leading to the development of bad habits and mental diseases. In essence, it's the lack of self-control and the giving in to negative impulses that fuels these emotions.\\n\\n(Ah, I didn't see a direct answer in the provided context, but I made an educated inference based on the discussions about habits, self-control, and negative emotions.)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}