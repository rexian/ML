{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNddmOX+Zk6cjrefVlVyYb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rexian/ML/blob/main/langchain/langchain_groq_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core langgraph>0.2.27\n",
        "!pip install -qU \"langchain[groq]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmR7t3thiWCp",
        "outputId": "53d05eb7-9773-432f-dfa6-032d82c71e35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/124.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter Your Groq API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JYFAjTei1KT",
        "outputId": "c9692cf1-9e06-4231-cfbd-f8e6da278dc4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Your Groq API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k1NdW_yHgYYP"
      },
      "outputs": [],
      "source": [
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"How do I become an AI Engineer?\"\n",
        "groq_response = model.invoke(prompt)\n",
        "print(groq_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YaEmKjwkDTX",
        "outputId": "79653a28-5221-41a8-8c9d-58aa29c3d3a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Becoming an AI Engineer requires a combination of education, skills, and experience in the fields of artificial intelligence, machine learning, and software engineering. Here are some steps to help you get started:\n",
            "\n",
            "1. **Education**:\n",
            "\t* Earn a Bachelor's or Master's degree in a relevant field such as Computer Science, Electrical Engineering, Mathematics, or Statistics.\n",
            "\t* Online courses and certifications can also be beneficial, especially for those who already have a degree in a related field.\n",
            "2. **Programming skills**:\n",
            "\t* Proficiency in at least one programming language, such as Python, Java, C++, or R.\n",
            "\t* Familiarity with machine learning libraries and frameworks, such as TensorFlow, PyTorch, or scikit-learn.\n",
            "3. **Mathematics and statistics**:\n",
            "\t* Strong understanding of linear algebra, calculus, probability, and statistics.\n",
            "\t* Familiarity with mathematical concepts, such as optimization, graph theory, and dynamical systems.\n",
            "4. **Machine learning and AI**:\n",
            "\t* Study machine learning algorithms, such as supervised and unsupervised learning, neural networks, and deep learning.\n",
            "\t* Familiarity with AI concepts, such as natural language processing, computer vision, and robotics.\n",
            "5. **Software development**:\n",
            "\t* Experience with software development methodologies, such as Agile and Scrum.\n",
            "\t* Familiarity with version control systems, such as Git.\n",
            "6. **Data analysis and visualization**:\n",
            "\t* Knowledge of data analysis and visualization tools, such as pandas, NumPy, Matplotlib, and Seaborn.\n",
            "\t* Experience with data visualization libraries, such as Tableau or Power BI.\n",
            "7. **Cloud computing**:\n",
            "\t* Familiarity with cloud computing platforms, such as AWS, Google Cloud, or Microsoft Azure.\n",
            "\t* Knowledge of cloud-based services, such as machine learning APIs and data storage solutions.\n",
            "8. **Networking and collaboration**:\n",
            "\t* Join online communities, such as Kaggle, Reddit's r/MachineLearning, or GitHub, to connect with other AI engineers and stay updated on industry trends.\n",
            "\t* Participate in hackathons, competitions, or research projects to gain experience and build your portfolio.\n",
            "9. **Certifications and training**:\n",
            "\t* Consider obtaining certifications, such as the Certified Data Scientist (CDS) or Certified Machine Learning Engineer (CMLE).\n",
            "\t* Take online courses or attend workshops and conferences to stay up-to-date with the latest AI and machine learning techniques.\n",
            "10. **Build a portfolio**:\n",
            "\t* Develop personal projects or contribute to open-source projects to demonstrate your skills and experience.\n",
            "\t* Create a portfolio that showcases your projects, including code, documentation, and results.\n",
            "11. **Stay updated**:\n",
            "\t* Follow industry leaders, researchers, and influencers on social media.\n",
            "\t* Stay current with the latest research, papers, and breakthroughs in AI and machine learning.\n",
            "12. **Pursue a graduate degree (optional)**:\n",
            "\t* Consider pursuing a Master's or Ph.D. in AI, machine learning, or a related field to gain advanced knowledge and credentials.\n",
            "\n",
            "Remember, becoming an AI Engineer requires dedication, persistence, and continuous learning. Focus on building a strong foundation in AI, machine learning, and software engineering, and stay updated with the latest developments in the field.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Prompt Fundamentals in Language Models*\n",
        "\n",
        "   - **Role of Prompts**: Set the stage for AI responses, from sentence completions to Q&A.\n",
        "   - **Impact**: Determines how the AI will reply.\n",
        "\n",
        "# *Designing Effective Prompts*\n",
        "\n",
        "   - **Key Elements**: Clarity, context, user query, and a signal for AI to respond.\n",
        "\n",
        "   - **Goal**: Direct AI towards the intended response.\n",
        "\n",
        "# *Using Prompt Templates*\n",
        "   - **Function**: Acts as a blueprint for crafting consistent, effective prompts.\n",
        "   - **Advantage**: Ensures AI receives appropriate input for the expected output.\n",
        "\n",
        "# *Simplicity in Explanation*\n",
        "   - Concise and straightforward, making the concept easy to understand without deep technical details."
      ],
      "metadata": {
        "id": "xhX2RvYUmMH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Define a simple prompt template\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"\"\"\n",
        "Human: What is the capital of {place}?\n",
        "AI: The capital of {place} is {capital}\n",
        "\"\"\")\n",
        "\n",
        "prompt = prompt_template.format(place=\"California\", capital=\"Sacramento\")\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxvRcBBxmMtq",
        "outputId": "1e0cbdbd-ba67-481a-bc6f-d0a469062bfe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Human: What is the capital of California?\n",
            "AI: The capital of California is Sacramento\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# No Input Variable\n",
        "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\n",
        "print(no_input_prompt.format())\n",
        "response = model.invoke(no_input_prompt.format())\n",
        "print(response.content)\n",
        "\n",
        "# One Input Variable\n",
        "one_input_prompt = PromptTemplate(input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke.\")\n",
        "print(one_input_prompt.format(adjective=\"funny\"))\n",
        "\n",
        "response = model.invoke(one_input_prompt.format(adjective=\"funny\"))\n",
        "print(response.content)\n",
        "\n",
        "\n",
        "# Multiple Input Variables\n",
        "multiple_input_prompt = PromptTemplate(\n",
        " input_variables=[\"adjective\", \"content\"],\n",
        " template=\"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "\n",
        "multiple_input_prompt = multiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")\n",
        "print(multiple_input_prompt)\n",
        "response = model.invoke(multiple_input_prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXVetfhjnCIu",
        "outputId": "03da5bd9-6b3f-481d-fcb0-9d3e62d081f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a joke.\n",
            "Why couldn't the bicycle stand up by itself?\n",
            "\n",
            "Because it was two-tired!\n",
            "Tell me a funny joke.\n",
            "Here's one:\n",
            "\n",
            "Why couldn't the bicycle stand up by itself?\n",
            "\n",
            "(wait for it...)\n",
            "\n",
            "Because it was two-tired!\n",
            "\n",
            "Hope that made you laugh!\n",
            "Tell me a funny joke about chickens.\n",
            "Here's one:\n",
            "\n",
            "Why did the chicken go to the doctor?\n",
            "\n",
            "Because it had a fowl cough!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate.from_template(\n",
        "    template=\"Write a {length} story about: {content}\"\n",
        ")\n",
        "\n",
        "prompt = prompt_template.format(\n",
        "    length=\"2-sentence\",\n",
        "    content=\"Sunnyvale, CA, the Silicon Valley of the USA\"\n",
        ")\n",
        "\n",
        "response = model.invoke(input=prompt)\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No0oihBnveeU",
        "outputId": "9e2b73eb-a04b-47d9-a88f-bee2f49ffa30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As the sun rose over the rows of sleek, modern homes in Sunnyvale, CA, software engineer Rachel stepped out of her front door to grab a coffee and catch up on the latest tech news, feeling grateful to call this hub of innovation her home. Little did she know, her latest project would soon be revolutionizing the way people communicated, cementing Sunnyvale's reputation as the Silicon Valley of the USA.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output parsers\n",
        "\n",
        "- Output parsers shape the AI model output to an organized format\n",
        "- They turn a block of text into organized data.\n",
        "- They can guide the AI on how to format its responses for consistency and ease of use."
      ],
      "metadata": {
        "id": "VTmCv9T1wWC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers.list import ListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List 5 {things}.\\n{format_instructions}\",\n",
        "    input_variables=[\"things\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions})\n",
        "\n",
        "output = model.invoke(input=prompt.format(things=\"deep sea fishes\"))\n",
        "\n",
        "print(output.content)\n",
        "\n",
        "output_parser.parse(str(output.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEO75UMrnK5v",
        "outputId": "59080d99-bb44-45cf-816f-4e3760faffd8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
            "Anglerfish, Viperfish, Fangtooth, Gulper Eel, Blobfish\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Anglerfish', 'Viperfish', 'Fangtooth', 'Gulper Eel', 'Blobfish']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain Expression Language (LCEL) Overview**:\n",
        "   - **Purpose**: Simplify building complex chains from basic components.\n",
        "   - **Features**: Supports streaming, parallelism, and logging.\n",
        "\n",
        "### Basic Use Case: Prompt + Model + Output Parser\n",
        "   - **Common Approach**: Link a prompt template with a model.\n",
        "   - **Chain Mechanism**: Using the `|` symbol, like a Unix pipe, to connect components.\n",
        "   - **Process Flow**: User input → Prompt Template → Model → Output Parser.\n",
        "\n",
        "### Understanding the Components\n",
        "   - **Step-by-Step**:\n",
        "     - User input is processed by the prompt template.\n",
        "     - Prompt template's output goes to the model.\n",
        "     - Model's output is refined by the output parser.\n",
        "   - **Example Code**: `chain = prompt | model | output_parser` shows how to combine components into a single LCEL chain."
      ],
      "metadata": {
        "id": "Rw7WNywZ0BpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Write a bengali poem about {topic}\")\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "chain.invoke({\"topic\": \"sunny days in Kolkata\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "sp6_Ozm50W0i",
        "outputId": "0722323f-07b5-4dd9-d3c5-2244fece0f1f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"সূর্যাস্তেকালে কলকাতা\\n\\nসূর্য উদিত হয়েছে শহরের আকাশে\\nকলকাতা সূর্যাস্তেকালে যথেষ্ট পরিবর্তন আনে\\nশহরের রাস্তা সবুজ হয়ে উঠে\\nপাখিরা সুর করে গায়\\n\\nসূর্যাস্তেকালে কলকাতা\\nআকাশে মহাসমুদ্র প্রস্থিত আকাশ\\nসূর্যাস্তেকালে কলকাতা\\nসবুজ পার্কে সামাজিক সমাজ\\n\\nসূর্যাস্তেকালে কলকাতা\\nশহরের রাস্তা সবুজ হয়ে উঠে\\nকলকাতা সূর্যাস্তেকালে\\nআকাশে মহাসমুদ্র প্রস্থিত আকাশ\\n\\nTranslation:\\n\\nSunny hours in Kolkata\\n\\nThe sun rises in the city's sky\\nKolkata in the sunny hours undergoes a significant change\\nThe city's roads turn green\\nThe birds sing sweet melodies\\n\\nIn the sunny hours of Kolkata\\nThe sky stretches out like the vast ocean\\nIn the sunny hours of Kolkata\\nThe social gatherings take place in the green parks\\n\\nIn the sunny hours of Kolkata\\nThe city's roads turn green\\nKolkata in the sunny hours\\nThe sky stretches out like the vast ocean\\n\\nNote: Bengali is a highly inflected language with a complex grammar system, and direct translation may not always convey the same nuance and beauty as the original poem.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}